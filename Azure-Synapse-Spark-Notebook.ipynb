{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Synapse Analytics & Spark - Complete Portfolio\n",
    "\n",
    "**Consolidated Notebook for Azure DP-203 Certification**\n",
    "\n",
    "This notebook consolidates 4 key Azure Synapse and Spark notebooks:\n",
    "1. Serverless SQL Pool Analysis\n",
    "2. Spark Data Analysis with Schema\n",
    "3. Spark Data Transformations & Partitioning\n",
    "4. Delta Lake Implementation with Time Travel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Serverless SQL Pool to Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product data from Azure Data Lake\n",
    "df = spark.read.load('abfss://files@datalaketx24ncr.dfs.core.windows.net/product_data/products.csv',\n",
    "                     format='csv', header=True)\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group products by category\n",
    "df_counts = df.groupby(df.Category).count()\n",
    "display(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas for data analysis\n",
    "import pandas as pd\n",
    "df_pd = pd.read_csv('abfss://files@datalaketx24ncr.dfs.core.windows.net/product_data/products.csv')\n",
    "(\n",
    "    df_pd\n",
    "    .groupby('Category')\n",
    "    .agg(productCount = ('ProductID','count'))\n",
    "    .reset_index()\n",
    "    .sort_values(by = ['productCount'], ascending = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Spark Schema Analysis & Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define explicit schema\n",
    "OrderSchema = StructType([\n",
    "    StructField('SalesOrderNumber', StringType()),\n",
    "    StructField('SalesOrderLineNumber', IntegerType()),\n",
    "    StructField('OrderDate', DateType()),\n",
    "    StructField('CustomerName', StringType()),\n",
    "    StructField('Email', StringType()),\n",
    "    StructField('Item', StringType()),\n",
    "    StructField('Quantity', IntegerType()),\n",
    "    StructField('UnitPrice', FloatType()),\n",
    "    StructField('Tax', FloatType())\n",
    "])\n",
    "\n",
    "# Load with schema\n",
    "df = spark.read.load('abfss://files@datalakey8hs4l2.dfs.core.windows.net/sales/orders/*.csv',\n",
    "                     format='csv', schema=OrderSchema)\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer analysis\n",
    "customers = df.select('CustomerName', 'Email')\n",
    "print(f'Total records: {customers.count()}')\n",
    "print(f'Distinct customers: {customers.distinct().count()}')\n",
    "display(customers.distinct().limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by year\n",
    "(\n",
    "    df\n",
    "    .select(year('OrderDate').alias('year'), 'Quantity', 'UnitPrice', 'Tax')\n",
    "    .groupBy('year')\n",
    "    .agg(expr(\"round(sum(Quantity*UnitPrice) + sum(Tax),2)\").alias('GrossRevenue'))\n",
    "    .orderBy(desc('year'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp view\n",
    "df.createOrReplaceTempView('salesorders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT YEAR(OrderDate) AS OrderYear,\n",
    "       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\n",
    "FROM salesorders\n",
    "GROUP BY YEAR(OrderDate)\n",
    "ORDER BY OrderYear;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Spark Data Transformations & Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Load order data\n",
    "order_details = spark.read.csv('/data/*.csv', header=True, inferSchema=True)\n",
    "display(order_details.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: Split customer name\n",
    "transformed_df = (\n",
    "    order_details\n",
    "    .withColumn('FirstName', f.split(f.col('CustomerName'), ' ').getItem(0))\n",
    "    .withColumn('LastName', f.split(f.col('CustomerName'), ' ').getItem(1))\n",
    "    .drop('CustomerName')\n",
    ")\n",
    "display(transformed_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet\n",
    "transformed_df.coalesce(1).write.mode('overwrite').parquet('/transformed_data/orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by year and month\n",
    "dated_df = (\n",
    "    transformed_df\n",
    "    .withColumn('Year', f.year(f.col('OrderDate')))\n",
    "    .withColumn('Month', f.month(f.col('OrderDate')))\n",
    ")\n",
    "\n",
    "dated_df.write.partitionBy('Year', 'Month').mode('Overwrite').parquet(\n",
    "    'abfss://files@datalakebtaj9gv.dfs.core.windows.net/partitioned_data/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external table\n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS sales')\n",
    "order_details.coalesce(1).write.saveAsTable(\n",
    "    'sales.sales_orders',\n",
    "    format='parquet',\n",
    "    mode='overwrite',\n",
    "    path='/sales_orders_table'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM sales.sales_orders LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Delta Lake Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product data\n",
    "df = spark.read.load('abfss://files@datalakemxp43de.dfs.core.windows.net/products/products.csv',\n",
    "                     format='csv', header=True)\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta table\n",
    "delta_table_path = '/delta/products-delta'\n",
    "df.write.format('delta').save(delta_table_path)\n",
    "print('Delta table created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Create DeltaTable object and update\n",
    "deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
    "deltaTable.update(\n",
    "    condition='ProductID==771',\n",
    "    set={'ListPrice': 'ListPrice * 0.9'}\n",
    ")\n",
    "display(deltaTable.toDF().limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time travel - access previous version\n",
    "previous_df = spark.read.format('delta').option('versionAsOf', 0).load(delta_table_path)\n",
    "display(previous_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View transaction history\n",
    "deltaTable.history(10).show(20, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create catalog table\n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS AdventureWorks')\n",
    "spark.sql(f\"CREATE TABLE AdventureWorks.ProductsExternal USING DELTA LOCATION '{delta_table_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE AdventureWorks;\n",
    "SELECT * FROM ProductsExternal LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This consolidated notebook demonstrates:\n",
    "\n",
    "1. **Serverless SQL Pool**: Loading and analyzing data\n",
    "2. **Schema Enforcement**: Defining explicit schemas\n",
    "3. **Data Transformations**: Column operations and aggregations\n",
    "4. **Partitioning**: Optimizing storage and query performance\n",
    "5. **Delta Lake**: ACID transactions and time travel\n",
    "6. **SQL Integration**: Queries and external tables\n",
    "\n",
    "**Technologies**: Azure Synapse | Apache Spark | Delta Lake | PySpark | SQL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
